#!/usr/bin/env python
#=============================================================================#
#                                                                             #
# The MIT License (MIT)                                                       #
#                                                                             #
# Copyright (c) 2018 Anita Petzler                                            #
#                                                                             #
# Permission is hereby granted, free of charge, to any person obtaining a     #
# copy of this software and associated documentation files (the "Software"),  #
# to deal in the Software without restriction, including without limitation   #
# the rights to use, copy, modify, merge, publish, distribute, sublicense,    #
# and/or sell copies of the Software, and to permit persons to whom the       #
# Software is furnished to do so, subject to the following conditions:        #
#                                                                             #
# The above copyright notice and this permission notice shall be included in  #
# all copies or substantial portions of the Software.                         #
#                                                                             #
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR  #
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,    #
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE #
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER      #
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING     #
# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER         #
# DEALINGS IN THE SOFTWARE.                                                   #
#                                                                             #
#=============================================================================#


from numpy.linalg import lstsq
from scipy import stats
from scipy import sparse
from scipy.interpolate import interp1d
from scipy.sparse.linalg import cg 
from scipy.sparse.linalg import LinearOperator as lo
from statistics import mean
import corner
import emcee
import idlsave
import matplotlib.pyplot as plt
import multiprocessing
import numpy as np
import pickle
import random
import scipy
import signal
import time

# Identify Gaussians
def IdentifyGaussians(data, fwhm, dv):
	'''
	Uses an autonomous Gaussian decomposition from Bob Lindner's Gausspy to identify potential features.
	Returns a list of grouped velocities.
	Parameters:
	data - dictionary object generated by Main()
	fwhm - estimate of the minimum full width at half-maximum of features expected in the data in km/sec.
			Used when categorising features as isolated or blended.
	dv - velocity resolution determined by Main() from average of main line (1665, 1667) resolutions.
			Used when merging features identified across the four OH transitions.

	Returns nested list of velocities identified by AGD algorithm:
		reduced_vel_list = [[v1], [v2, v3], [v4]]
		
		where v1 and v4 are isolated features that can be fit independently, but v2 and v3 are close 
		enough in velocity that they must be fit together as a blended feature.
	'''
	source_name = data['source_name']

	vel_axis_1612 = data['vel_axis']['1612']
	vel_axis_1665 = data['vel_axis']['1665']
	vel_axis_1667 = data['vel_axis']['1667']
	vel_axis_1720 = data['vel_axis']['1720']

	data['spectrum']['1612'] = np.nan_to_num(data['spectrum']['1612'])
	data['spectrum']['1665'] = np.nan_to_num(data['spectrum']['1665'])
	data['spectrum']['1667'] = np.nan_to_num(data['spectrum']['1667'])
	data['spectrum']['1720'] = np.nan_to_num(data['spectrum']['1720'])
	
	# get a range of velocity guesses from Bob's code
	master_vel_list = []
	for freq in ['1612', '1665', '1667', '1720']: # '1612', '1665', '1667', '1720'
		
		vel_axis = data['vel_axis'][freq]
		spectrum = data['spectrum'][freq]
		
		initial_vel_guesses = BatchAGD(source_name, freq, vel_axis, spectrum, data['rms'][freq])
		master_vel_list = np.append(master_vel_list, initial_vel_guesses)

	# merge into blended and isolated components
	reduced_vel_list = ReduceList(master_vel_list, 3.0 * dv, 10.0 * fwhm)
	with open('pickles/' + source_name + 'reduced_vel_list.pickle', 'w') as f:
		pickle.dump((data, reduced_vel_list), f)
	
	return reduced_vel_list
def BatchAGD(source_name, freq, vel_axis, spectrum, rms):
	'''
	Perform autonomous Gaussian decomposition (AGD) on given sightline and frequency for the alpha values 
	defined by min_alpha, max_alpha, alpha_step. If the program finds too many 'features' that are actually 
	noise, remove the lower alpha values. If it finds too many wide features, remove the higher alpha numbers.
	Will save velocity lists as pickles then merge the lists from the range of alpha values. 
	Parameters:
	source_name - unique identifier for sightline to be used when saving pickles during Gaussian decomposition 
			process.
	freq - string used to identify the OH transition ('1612', '1665', '1667' or '1720')
	vel_axis - velocity axis
	spectrum - transition spectrum (i.e. brightness temperature, optical depth)
	rms - root mean square error of spectrum. Used by autonomous Gaussian decomposition algorithm

	Returns a list of initial guesses of centroid velocity for a range of alpha values. Here alpha is a proxy 
	for feature width. 
	'''
	alpha_array = np.arange(-0.5, 2, 0.5)
	master_vel_list = []
	for alpha in alpha_array:
		if abs(alpha) > 0.01:
			vel_list = RunAGD(source_name, freq, alpha, vel_axis, spectrum, rms)
			master_vel_list = np.append(master_vel_list, vel_list)
	master_vel_list = np.sort(master_vel_list)
	return master_vel_list
def RunAGD(source_name, freq, alpha, vel_axis, spectrum, rms):
	'''
	Uses regularised differentiation algorithm borrowed from Bob Lindner's Gausspy, identifying minima in the 
	2nd derivative as potential locations of Gaussian emission features. It also performs this process on 
	-1 * spectrum to identify absorption features. Returns a list of these velocities.
	Parameters:
	source_name - unique identifier for sightline to be used when saving pickles during Gaussian decomposition 
			process.
	freq - string used to identify the OH transition ('1612', '1665', '1667' or '1720')
	alpha - log(alpha) parameter use in regularised differentiation (see Tikhonov 1963 'Reviews of Topical 
			Problems: Peaks of Random Processes' Soviet Physics Uspekhi 5, 594, and Lindner et al. 2015 AJ 
			149, 138.)
	vel_axis - velocity axis
	spectrum - transition spectrum (i.e. brightness temperature, optical depth)
	rms - root mean square error of spectrum. Used by autonomous Gaussian decomposition algorithm

	Returns a list of initial guesses of centroid velocity for a range of alpha values. Here alpha is a proxy 
	for feature width. 
	'''
	spectrum_pos = spectrum 
	spectrum_neg = -1.0 * spectrum_pos

	# Set up data
	data_pos = {}
	data_neg = {}
	chan = vel_axis
	errors = np.ones(len(vel_axis)) * rms

	# Enter data into AGD dataset
	data_pos['data_list'] = data_pos.get('data_list', []) + [spectrum_pos]
	data_neg['data_list'] = data_neg.get('data_list', []) + [spectrum_neg]
	data_pos['x_values'] = data_pos.get('x_values', []) + [chan]
	data_neg['x_values'] = data_neg.get('x_values', []) + [chan]
	data_pos['errors'] = data_pos.get('errors', []) + [errors]
	data_neg['errors'] = data_neg.get('errors', []) + [errors]
	FILENAME_DATA_POS = 'pickles/' + source_name + '_' + freq + '_' + str(alpha) + '_pos.pickle'
	FILENAME_DATA_NEG = 'pickles/' + source_name + '_' + freq + '_' + str(alpha) + '_neg.pickle'
	with open(FILENAME_DATA_POS, 'w') as f:
		pickle.dump(data_pos, f)
	with open(FILENAME_DATA_NEG, 'w') as f:
		pickle.dump(data_neg, f)
	snr_thresh = 3 * float(rms)

	# Load GaussPy
	g = GaussianDecomposer()

	# Setting AGD parameters
	g.set('phase', 'one')
	g.set('SNR_thresh', [0 - snr_thresh, snr_thresh])
	g.set('alpha1', alpha)

	# Run GaussPy
	data_decomp_pos = g.batch_decomposition(FILENAME_DATA_POS)
	data_decomp_neg = g.batch_decomposition(FILENAME_DATA_NEG)
	all_means = AddDataDecomp(data_decomp_pos, data_decomp_neg)
	vel_list = all_means

	return (vel_list)

# vvv Bob Lindner's code vvv
class GaussianDecomposer(object):
	def __init__(self, filename = None, phase = 'one'):
		if filename:
			with open(filename) as f:
				temp = pickle.load(f)
			self.p = temp.p
		else:			
			self.p = {'alpha1':None, 'alpha2':None, 'training_results':None,
						'phase':'one', 'SNR2_thresh':5., 'SNR_thresh':5., 'deblend':True,
						'mode':'c', 'BLFrac':0.1, 'verbose':False, 'plot':False, 'perform_final_fit':True}
	def decompose(self, xdata, ydata, edata):
		''' Decompose a single spectrum using current parameters '''
		if ((self.p['phase'] == 'one') and (not self.p['alpha1'])):
			print 'phase = one, and alpha1 is unset'
			return

		if  (self.p['phase'] == 'two') and ((not self.p['alpha1']) or (not self.p['alpha2'])):
			print 'phase = two, and either alpha1 or alpha2 is unset'
			return

		if self.p['mode'] != 'conv':
			a1 = 10**self.p['alpha1']
			a2 = 10**self.p['alpha2'] if self.p['phase']=='two' else None
		else:
			a1 = self.p['alpha1']
			a2 = self.p['alpha2'] if self.p['phase']=='two' else None

		# status, results = AGD_decomposer.AGD(xdata, ydata, edata, 
		results = AGD(xdata, ydata, edata, 
											 alpha1 = a1, alpha2 = a2, 
											 phase = self.p['phase'], mode = self.p['mode'], 
											 verbose = self.p['verbose'], SNR_thresh = self.p['SNR_thresh'], 
											 BLFrac = self.p['BLFrac'], SNR2_thresh = self.p['SNR2_thresh'], 
											 deblend = self.p['deblend'], perform_final_fit = self.p['perform_final_fit'], 
											 plot = self.p['plot']) 
		return results
	def set(self, key, value):
		if key in self.p:
			self.p[key] = value
		else:
			print 'Given key does not exist.'
	def batch_decomposition(self, science_data_path, ilist=None):
		''' Science data sould be AGD format 
			ilist is either None or an integer list'''
		# Dump information to hard drive to allow multiprocessing
		with open('batchdecomp_temp.pickle','w') as f:
			pickle.dump([self, science_data_path, ilist], f)
		init()
		result_list = func()

		return result_list
def initialGuess(vel, data, errors = None, alpha = None, plot = False, mode ='python',verbose = False, SNR_thresh = 5.0, BLFrac = 0.1, SNR2_thresh = 5.0, deblend = True):
	'''  Find initial parameter guesses (AGD algorithm)

	data,			 Input data 
	dv,			 x-spacing absolute units 
	alpha = No Default,	 regularization parameter 
	plot = False,	 Show diagnostic plots?
	verbose = True	Diagnostic messages
	SNR_thresh = 5.0  Initial Spectrum S/N threshold
	BLFrac =		  Edge fraction of data used for S/N threshold computation
	SNR2_thresh =   S/N threshold for Second derivative
	mode = Method for taking derivatives

	# LEGAL:
	#  Copyright notice:
	# Copyright 2010. Los Alamos National Security, LLC. This material
	# was produced under U.S. Government contract DE-AC52-06NA25396 for
	# Los Alamos National Laboratory, which is operated by Los Alamos
	# National Security, LLC, for the U.S. Department of Energy. The
	# Government is granted for, itself and others acting on its
	# behalf, a paid-up, nonexclusive, irrevocable worldwide license in
	# this material to reproduce, prepare derivative works, and perform
	# publicly and display publicly. Beginning five (5) years after
	# (March 31, 2011) permission to assert copyright was obtained,
	# subject to additional five-year worldwide renewals, the
	# Government is granted for itself and others acting on its behalf
	# a paid-up, nonexclusive, irrevocable worldwide license in this
	# material to reproduce, prepare derivative works, distribute
	# copies to the public, perform publicly and display publicly, and
	# to permit others to do so. NEITHER THE UNITED STATES NOR THE
	# UNITED STATES DEPARTMENT OF ENERGY, NOR LOS ALAMOS NATIONAL
	# SECURITY, LLC, NOR ANY OF THEIR EMPLOYEES, MAKES ANY WARRANTY,
	# EXPRESS OR IMPLIED, OR ASSUMES ANY LEGAL LIABILITY OR
	# RESPONSIBILITY FOR THE ACCURACY, COMPLETENESS, OR USEFULNESS OF
	# ANY INFORMATION, APPARATUS, PRODUCT, OR PROCESS DISCLOSED, OR
	# REPRESENTS THAT ITS USE WOULD NOT INFRINGE PRIVATELY OWNED
	# RIGHTS. 
	#
	# BSD License notice:
	# Redistribution and use in source and binary forms, with or without
	# modification, are permitted provided that the following conditions
	# are met: 
	# 
	#      Redistributions of source code must retain the above
	#      copyright notice, this list of conditions and the following
	#      disclaimer.  
	#      Redistributions in binary form must reproduce the above
	#      copyright notice, this list of conditions and the following
	#      disclaimer in the documentation and/or other materials
	#      provided with the distribution. 
	#      Neither the name of Los Alamos National Security nor the names of its
	#      contributors may be used to endorse or promote products
	#      derived from this software without specific prior written
	#      permission. 
	#  
	# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND
	# CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES,
	# INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
	# MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
	# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR
	# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
	# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
	# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF
	# USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED
	# AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
	# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
	# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
	# POSSIBILITY OF SUCH DAMAGE. 
	#
	# Code begins here:

	'''
	errors = None # Until error

	if not alpha:
		print 'Must choose value for alpha, no default.'
		return

	if np.any(np.isnan(data)):
		print 'NaN-values in data, cannot continue.'
		return

	# Data inspection
	vel = np.array(vel)
	data = np.array(data)
	dv = np.abs(vel[1]-vel[0])
	fvel = interp1d(np.arange(len(vel)),vel) # Converts from index -> x domain
	data_size = len(data)

	# Take regularized derivatives
	t0 = time.time()

	u =  np.nan_to_num(TVdiff(data, dx = dv, alph = alpha))
	u2 = np.nan_to_num(TVdiff(u, dx = dv, alph = alpha))
	u3 = np.nan_to_num(TVdiff(u2, dx = dv, alph = alpha))
	u4 = np.nan_to_num(TVdiff(u3, dx = dv, alph = alpha))

	# Decide on signal threshold
	if not errors:
		errors = np.std(data[0:int(BLFrac * data_size)])   

	thresh = SNR_thresh * errors
	mask1 = np.array(data > thresh, dtype = 'int')[1:] # Raw Data S/N 
	mask3 = np.array(u4.copy()[1:] > 0., dtype = 'int')	 # Positive 4th derivative
 
	if SNR2_thresh > 0.:
		wsort = np.argsort(np.abs(u2))
		RMSD2 = np.std(u2[wsort[0:int(0.5*len(u2))]]) / 0.377 # RMS based in +-1 sigma fluctuations
		thresh2 = -RMSD2 * SNR2_thresh
	else:
		thresh2 = 0.
	mask4 = np.array(u2.copy()[1:] < thresh2, dtype='int') # Negative second derivative

	# Find optima of second derivative
	# --------------------------------
	zeros = np.abs(np.diff(np.sign(u3)))
	zeros = zeros * mask1 *  mask3 * mask4
	offsets_data_i = np.array(np.where(zeros)).ravel() # Index offsets	
	offsets = fvel(offsets_data_i + 0.5) # Velocity offsets (Added 0.5 July 23)
	N_components = len(offsets)

	# Check if nothing was found, if so, return null
	# ----------------------------------------------
	if N_components == 0:
		odict = {'means':[], 'FWHMs': [], 'amps':[],
			 'u2':u2, 'errors':errors, 'thresh2':thresh2, 
			 'thresh':thresh, 'N_components':N_components}

		return odict

	# Find points of inflection
	inflection = np.abs(np.diff(np.sign(u2)))	

	# Find Relative widths, then measure
	# peak-to-inflection distance for sharpest peak
	widths = np.sqrt(np.abs(data/u2)[offsets_data_i])
	FWHMs = widths * 2.355

	# Attempt deblending.
	# If Deblending results in all non-negative answers, keep.
	amps = np.array(data[offsets_data_i])
	if deblend:
		FF_matrix = np.zeros([len(amps),len(amps)])
		for i in range(FF_matrix.shape[0]):
			for j in range(FF_matrix.shape[1]):
				FF_matrix[i,j] = np.exp(-(offsets[i]-offsets[j])**2/2./(FWHMs[j] / 2.355)**2)
		amps_new = lstsq(FF_matrix,amps)[0]
		if np.all(amps_new > 0):
			amps = amps_new

	odict = {'means':offsets, 'FWHMs': FWHMs, 'amps':amps, 
			 'u2':u2, 'errors':errors, 'thresh2':thresh2, 
			 'thresh':thresh, 'N_components':N_components}

	return odict
def TVdiff(data, dx = 1.0, alph = 0.1, beta = 0.1, thresh = 1E-4, max_iter=20):
	dxi = dx
	dx = 1.0 
	# Data scaling
	scale_data = 1.0 / np.max(data)

	data = data * scale_data
   
	# Get length of data
	n = len(data) # Number of data points 
	data = np.matrix(np.reshape(data,[n,1]))	
	
	# Construct sparse differentiation matrix
	
	c = np.ones([n + 1]) / dx
	D = sparse.spdiags([-c, c], [0, 1], n, n + 1)
	D_initial = sparse.spdiags([-c, c], [0, 1], n - 1, n)
	DT = D.transpose()
	
	# Define anti-derivatives A and ATranspose
	def A(x):
		'''Antidifferentiation matrix'''
		out = np.cumsum(x).reshape([len(x),1]) - 0.5 * (x + x[0,0])
		return out[1:,:] * dx

	def AT(x):
		'''Transpose of antidifferentiation matrix'''
		out = np.sum(x) * np.ones([n + 1, 1]) 
		chunk1 = np.sum(x)
		chunk2 = np.cumsum(x).reshape([n,1]) - x / 2.
		out[0,0] = out[0,0] - chunk1
		out[1:,:] = out[1:,:] - chunk2
		out = out * dx
		return out
	
	# Initial guess for u is naive derivative
	u = np.matrix(np.zeros([n+1,1]))
	u[1:n] = D_initial * data
	
	# First data item
	ofst = data[0]
	ATb = AT(ofst - data)	
	
	# Main loop
	i = 0
	norm = 1.0
	while (norm > thresh) and (i < max_iter):
		diags = 1. / np.sqrt( np.array(D * u)**2 + beta**2)
		Q = sparse.spdiags(diags.ravel(), 0, n, n)
		# Hessian term
		L = dx * DT * Q * D
		# Gradient of functional.
		g = AT( A( u ) ) + ATb + alph * L * u
		def f(x):
			x = x.reshape([len(x),1])
			a = alph * L * x +  AT( A( x ) ) 
			return a
		operator = lo((n+1, n+1), f, dtype = 'float')   
		s = cg(operator,g)[0]
		
		s = np.reshape(s, [len(s), 1])
		u = u - s
		
		# Test the convergence condition
		s_norm = np.sqrt(np.sum(np.array(s).ravel()**2))
		u_norm = np.sqrt(np.sum(np.array(u).ravel()**2))
		norm = s_norm / u_norm
 
		i=i+1	
	u = u[1:] # Clip off the first element

	return np.array(u).ravel() /scale_data / dxi
def AGD(vel, data, errors, alpha1 = None, alpha2 = None,plot = False, mode ='python', verbose = False, SNR_thresh = 5.0, BLFrac = 0.1, SNR2_thresh=5.0, deblend=True,  perform_final_fit = True, phase='one'):
	''' Autonomous Gaussian Decomposition
	'''
	if type(SNR2_thresh) != type([]): SNR2_thresh = [SNR2_thresh, SNR2_thresh]
	if type(SNR_thresh) != type([]): SNR_thresh = [SNR_thresh, SNR_thresh]

	if (not alpha2) and (phase=='two'):
		print 'alpha2 value required'
		return

	dv = np.abs(vel[1] - vel[0])
	v_to_i = interp1d(vel, np.arange(len(vel)))

	#--------------------------------------#
	# Find phase-one guesses			   #
	#--------------------------------------#
	agd1 = initialGuess(vel, data, errors = None, alpha = alpha1, 
											  plot = plot, mode = mode, verbose = verbose, 
											  SNR_thresh = SNR_thresh[0], 
											  BLFrac = BLFrac, 
											  SNR2_thresh = SNR2_thresh[0], 
											  deblend = deblend)
	return agd1
def init_worker():
	''' Worker initializer to ignore Keyboard interrupt '''
	signal.signal(signal.SIGINT, signal.SIG_IGN)
def init():
	global agd_object, science_data_path, ilist, agd_data
	with open('batchdecomp_temp.pickle') as f:
		[agd_object, science_data_path, ilist] = pickle.load(f)
	with open(science_data_path) as f:
		agd_data = pickle.load(f)
	if ilist == None: ilist = np.arange(len(agd_data['x_values']))
def decompose_one(i): 
	result = GaussianDecomposer.decompose(agd_object, 
										  agd_data['x_values'][i], 
										  agd_data['data_list'][i], 
										  agd_data['errors'][i])
	return result
def func():
	ncpus = multiprocessing.cpu_count()
	p = multiprocessing.Pool(ncpus, init_worker)
	if agd_object.p['verbose']: print 'N CPUs: ', ncpus
	try:
		results_list = p.map(decompose_one, ilist)
	except KeyboardInterrupt:
		print "KeyboardInterrupt... quitting."
		p.terminate()
		quit()
	p.close()
	del p
	return results_list[0]
	return BIC_null
# ^^^ Bob Lindner's code ^^^

def AddDataDecomp(data_decomp_pos, data_decomp_neg):
	'''
	Concatenates positive decomposition with corrected negative decomposition from RunAGD()

	Returns a list of initial guesses of centroid velocities.
	'''
	pos_means = np.array(data_decomp_pos['means'])
	neg_means = np.array(data_decomp_neg['means'])
	all_means = np.array(np.append(pos_means, neg_means))
	
	return all_means
def ReduceList(master_list, merge_size, group_spacing):	
	'''
	Merges values in master_list separated by less than merge_size, and groups features separated by 
	less than group_spacing into blended features.
	Returns a list of lists: i.e. [[a], [b, c, d], [e]] where 'a' and 'e' are isolated
	features and 'b', 'c' and 'd' are close enough to overlap in velocity.

	Parameters:
	master_list - list of velocities
	merge_size - any velocities separated by less than this distance will be merged into one velocity. This 
			is performed in 4 stages, first using merge_size / 4 so that the closest velocities are merged 
			first. Merged velocities are replaced by their mean. Based on extensive testing with synthetic 
			spectra his has been set as 3 * the velocity resolution. If your data have been converted to 
			velocity spectra using imprecise rest frequencies you may wish to increase the value of 
			merge_size.
	group_spacing - any velocities separated by less than this value will be grouped together so they can 
			be fit as blended features. Based on extensive testing with synthetic spectra his has been set 
			as 10 * expected minimum fwhm. Smaller values are likely to prevent the accurate identification 
			of blended features, while larger values will increase running time.

	Returns nested list of velocities:
		reduced_vel_list = [[v1], [v2, v3], [v4]]
		
		where v1 and v4 are isolated features that can be fit independently, but v2 and v3 are close 
		enough in velocity that they must be fit together as a blended feature.
	'''
	master_list = np.sort(master_list)
	
	# Step 1: merge based on merge_size
	new_merge_list = master_list

	for merge in [merge_size / 4, 2 * merge_size / 4, 3 * merge_size / 4, merge_size]:
		new_merge_list = MergeFeatures(new_merge_list, merge, 'merge')
	
	# Step 2: identify components likely to overlap to be fit together
	final_merge_list = MergeFeatures(new_merge_list, group_spacing, 'group')

	return final_merge_list
def MergeFeatures(master_list, size, action):
	'''
	Does the work for ReduceList

	Parameters:
	master_list - list of velocities generated by AGD()
	size - Distance in km/sec for the given action
	action - Action to perform: 'merge' or 'group'

	Returns nested list of velocities:
		reduced_vel_list = [[v1], [v2, v3], [v4]]
		
		where v1 and v4 are isolated features that can be fit independently, but v2 and v3 are close 
		enough in velocity that they must be fit together as a blended feature.
	'''	
	new_merge_list = []
	check = 0
	while check < len(master_list):
		skip = 1
		single = True

		if action == 'merge':
			while check + skip < len(master_list) and master_list[check + skip] - master_list[check] < size:
				skip += 1
				single = False
			if single == True:
				new_merge_list = np.append(new_merge_list, master_list[check])
			else:
				new_merge_list = np.append(new_merge_list, mean(master_list[check:check + skip]))
			check += skip

		elif action == 'group':
			while check + skip < len(master_list) and master_list[check + skip] - master_list[check + skip - 1] < size:
				skip += 1
			new_merge_list.append(master_list[check:check + skip].tolist())
			check += skip
		else:
			print 'Error defining action in MergeFeatures'

	return new_merge_list
# Fit Gaussians
def BICnull(data):
	'''
	Finds Bayesian Information Criterion (approximation of the Bayes factor) of the null model (i.e. where 
	tau = 0 at all velocities). See R. E. Kass and A. E. Raftery 1995. 'Bayes factors' Journal of the 
	American Statistical Association 90, 773.

	Parameters:
	data - dictionary object generated by Main()

	Returns Bayesian Information Criterion (float) 
	'''
	spectrum_1612 = data['spectrum']['1612']
	spectrum_1665 = data['spectrum']['1665']
	spectrum_1667 = data['spectrum']['1667']
	spectrum_1720 = data['spectrum']['1720']
	n = len(spectrum_1612) + len(spectrum_1665) + len(spectrum_1667) + len(spectrum_1720)
	null_sse = sum((spectrum_1612)**2.) + sum((spectrum_1665)**2.) + sum((spectrum_1667)**2.) + sum((spectrum_1720)**2.)
	null_BIC = n * np.log(null_sse / n)
	return null_BIC
def MCMC(data, velocity, prev_BIC, fwhm, plot_num, save_as_name):
	'''
	Uses emcee (Markov Chain Monte Carlo algorithm) to simultaneously fit gaussians of a given centroid 
	velocity and fwhm to all four OH transitions. All models have 6n components where n is the number of 
	Gaussians: centroid velocity, fwhm, heights in the four OH transitions.

	NOTE: later versions of BGD will remove the use of MCMC as it's too computationally expensive and not 
	really necessary. It will likely be replaced by mpfit or similar.

	Parameters:
	data - dictionary object generated by Main()
	velocity - list of velocities at which to place a Gaussian
	prev_BIC - BIC of the previous model tested. Used to determine when to stop placing Gaussians
	fwhm - estimate of the minimum full width at half-maximum of features expected in the data in km/sec.
			Used to limit parameter space to 'reasonable' values.
	plot_num - number to uniquely identify generated plots
	save_as_name - name to uniquely identify sightlines in generated plots

	Returns:
	BIC_model - BIC of the identified model
	median_parameters - parameters determined by median of Markov chains
	Convergence_test - 'Pass' or 'Fail': Whether or not the inner 50% of values for centroid velocity and 
			fwhm in the final 50 iterations fall within a given test range.
	Skewness_test - 'Pass' or 'Fail': Whether or not the inner 80% of values for centroid velocity and 
			fwhm in the final 50 iterations have a skewness factor below a given threshold.
	'''
	vel_1612 = data['vel_axis']['1612']
	vel_1665 = data['vel_axis']['1665']
	vel_1667 = data['vel_axis']['1667']
	vel_1720 = data['vel_axis']['1720']

	spec_1612 = data['spectrum']['1612']
	spec_1665 = data['spectrum']['1665']
	spec_1667 = data['spectrum']['1667']
	spec_1720 = data['spectrum']['1720']

	source_name = data['source_name']

	num_Gauss = len(velocity)
	nwalkers = 50 * num_Gauss
	ndim = 6 * num_Gauss
	Convergence_test = 'Pass'
	Skewness_test = 'Pass'

	vel_tolerance = 2.0 * fwhm # limits how much the velocity can be moved from its initial value
	vel_convergence = 2.5 # range of inner 50%
	fwhm_convergence = 2.5 # range of inner 50%
	vel_skewness = 2. # of inner 80%
	fwhm_skewness = 2. # of inner 80%
	dBIC_threshold = -10.
	height_threshold = 1.0 # sigma value of at least one line in order to be considered

	burn_iterations = 1000
	iterations = 100
	
	# defining initial positions
	p0 = [velocity[0], 1.0, 0, 0, 0, 0]
	if num_Gauss > 1:
		for component in range(num_Gauss - 1):
			p0 = p0 + [velocity[component + 1], 1.0, 0, 0, 0, 0]
	# Then add a random offset for each walker
	p0 = [[np.random.randn() / 100 + y for y in p0] for x in range(nwalkers)]
	# and tweak where required
	for component in range(num_Gauss):
		for walker in range(nwalkers):
			p0[walker][component * 6 + 1] = abs(p0[walker][component * 6 + 1]) # makes fwhm positive
	
	# initiating and running mcmc
	sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args = [velocity, vel_tolerance, data], threads = multiprocessing.cpu_count())
	pos, prob, state = sampler.run_mcmc(p0, burn_iterations)

	# plot walker chains during burn in phase
	for parameter in range(ndim):
		plt.figure()

		for walker in range(sampler.chain.shape[0]):
			plt.plot(range(sampler.chain.shape[1]), sampler.chain[walker,:,parameter])
		plt.title(source_name + '_' + str(round(velocity[0], 1)) + ' for param ' + str(parameter) + ': burn in')
		plt.savefig('Chain_plots/burn_' + save_as_name + '_' + str(round(velocity[0], 1)) + '_' + str(parameter) + '_Plot_' + str(plot_num) + '.pdf')
		plt.close()

	########## v Convergence Test v ##########

	if ConvergenceTest(sampler.chain, num_Gauss, 0, vel_convergence) == 'Fail' or ConvergenceTest(sampler.chain, num_Gauss, 1, fwhm_convergence) == 'Fail':
		pos, prob, state = sampler.run_mcmc(pos, burn_iterations) # burns again
	if ConvergenceTest(sampler.chain, num_Gauss, 0, vel_convergence) == 'Fail' or ConvergenceTest(sampler.chain, num_Gauss, 1, fwhm_convergence) == 'Fail':
		Convergence_test = 'Fail'

	########## ^ Convergence Test ^ ##########

	# run final time
	sampler.reset()
	sampler.run_mcmc(pos, iterations)

	# plot walker chains during final phase
	median_parameters = np.ones(ndim)

	for parameter in range(ndim):
		plt.figure()
		for walker in range(sampler.chain.shape[0]):
			plt.plot(range(sampler.chain.shape[1]), sampler.chain[walker,:,parameter])
		plt.title(source_name + '_' + str(round(velocity[0], 1)) + ' for param ' + str(parameter) + ': burn in')
		plt.savefig('Chain_plots/final_' + save_as_name + '_' + str(round(velocity[0], 1)) + '_' + str(parameter) + '_Plot_' + str(plot_num) + '.pdf')
		plt.close()
		quantiles = corner.quantile(sampler.flatchain[:,parameter], [0.16, 0.50, 0.84])
		median_parameters[parameter] = quantiles[1]

	########### v Skewnesse Test v ###########

	if SkewnessTest(sampler.chain, num_Gauss, 0, vel_skewness) == 'Fail' or SkewnessTest(sampler.chain, num_Gauss, 1, fwhm_skewness) == 'Fail':
		Skewness_test = 'Fail'

	########### ^ Skewnesse Test ^ ###########
	
	BIC_model = BIC(median_parameters, data)
	(model_1612, model_1665, model_1667, model_1720) = MakeModel(median_parameters, vel_1612, vel_1665, vel_1667, vel_1720)
	
	rounded_velocity = [round(x, 2) for x in velocity]

	# save current model
	plt.figure()
	plt.subplot(511)
	plt.title(save_as_name + ' for ' + str(rounded_velocity))
	plt.plot(vel_1612, spec_1612, color = 'blue', label = '1612 MHz')
	plt.plot(vel_1612, model_1612, color = 'black')
	plt.subplot(512, sharex = plt.subplot(511))
	plt.plot(vel_1665, spec_1665, color = 'green', label = '1665 MHz')
	plt.plot(vel_1665, model_1665, color = 'black')
	plt.subplot(513, sharex = plt.subplot(511))
	plt.plot(vel_1667, spec_1667, color = 'red', label = '1667 MHz')
	plt.plot(vel_1667, model_1667, color = 'black')	
	plt.subplot(514, sharex = plt.subplot(511))
	plt.plot(vel_1720, spec_1720, color = 'cyan', label = '1720 MHz')
	plt.plot(vel_1720, model_1720, color = 'black')
	plt.subplot(515, sharex = plt.subplot(511))
	plt.plot(vel_1612, spec_1612 - model_1612, color = 'blue')
	plt.plot(vel_1665, spec_1665 - model_1665, color = 'green')
	plt.plot(vel_1667, spec_1667 - model_1667, color = 'red')
	plt.plot(vel_1720, spec_1720 - model_1720, color = 'cyan')
	if plot_num == 'Final':
		plt.savefig('Models/Final/' + save_as_name + '.pdf')
	else:
		plt.savefig('Models/' + save_as_name + '_Plot_' + str(plot_num) + '_model.pdf')
	# plt.show()
	plt.close()

	return BIC_model, median_parameters, Convergence_test, Skewness_test
def lnprob(x, velocity, vel_tolerance, data):
	'''
	Note: this is not a true log probability as defined by Bayes' Theorem. 
	'''
	if np.isfinite(lnprior(x, velocity, vel_tolerance, data)):
		BIC_null = BICnull(data)
		new_BIC = BIC(x, data)
		return -new_BIC
	else:
		return -np.inf
def lnprior(x, velocity, vel_tolerance, data):
	'''
	Note: this is not a true log prior as defined by Bayes' Theorem.
	limits parameters to 'reasonable' values
	'''
	spec_1612 = data['spectrum']['1612']
	spec_1665 = data['spectrum']['1665']
	spec_1667 = data['spectrum']['1667']
	spec_1720 = data['spectrum']['1720']

	test = True
	for component in range(len(x) / 6):
		if abs(x[component * 6] - velocity[component]) > vel_tolerance or x[component * 6 + 1] < 0.3 or x[component * 6 + 1] > 5: # check that velocities are still close to those identified, and that fwhm is reasonable
			test = False
			break
		elif abs(x[component * 6 + 2]) > 2 * max(abs(spec_1612)) or abs(x[component * 6 + 3]) > 2 * max(abs(spec_1665)) or abs(x[component * 6 + 4]) > 2 * max(abs(spec_1667)) or abs(x[component * 6 + 5]) > 2 * max(abs(spec_1720)): # these are heights, shouldn't be too high
			test = False
			break
	if test == True:
		return 0.0
	else:
		return -np.inf
def BIC(x, data):
	'''
	Returns the new BIC
	'''
	vel_1612 = data['vel_axis']['1612']
	vel_1665 = data['vel_axis']['1665']
	vel_1667 = data['vel_axis']['1667']
	vel_1720 = data['vel_axis']['1720']

	spec_1612 = data['spectrum']['1612']
	spec_1665 = data['spectrum']['1665']
	spec_1667 = data['spectrum']['1667']
	spec_1720 = data['spectrum']['1720']

	(model_1612, model_1665, model_1667, model_1720) = MakeModel(x, vel_1612, vel_1665, vel_1667, vel_1720)

	n = len(spec_1612) + len(spec_1665) + len(spec_1667) + len(spec_1720)
	sse_model = sum((spec_1612 - model_1612)**2.) + sum((spec_1665 - model_1665)**2.) + sum((spec_1667 - model_1667)**2.) + sum((spec_1720 - model_1720)**2.)
	k = len(x)
	BIC_model = n * np.log(sse_model / n) + k * np.log(n)

	return BIC_model
def MakeModel(x, vel_1612, vel_1665, vel_1667, vel_1720):
	'''
	Constructs a Gaussian model based on parameters given in x.

	Parameters:
	x - Parameters of Gaussian component(s): [vel_1, fwhm_1, height_1612_1, height_1665_1, height_1667_1, 
			height_1720_1, ..., _N] for N components
	vel_XXXX - velocity axes of the 4 OH transitions

	Returns Gaussian models for each of the 4 OH transitions
	'''
	model_1612 = np.zeros(len(vel_1612))
	model_1665 = np.zeros(len(vel_1665))
	model_1667 = np.zeros(len(vel_1667))
	model_1720 = np.zeros(len(vel_1720))

	for component in range(len(x) / 6): 
		[vel, fwhm, height_1612, height_1665, height_1667, height_1720] = x[component * 6:component * 6 + 6]

		model_1612 += Gaussian(vel, fwhm, height_1612)(vel_1612)
		model_1665 += Gaussian(vel, fwhm, height_1665)(vel_1665)
		model_1667 += Gaussian(vel, fwhm, height_1667)(vel_1667)
		model_1720 += Gaussian(vel, fwhm, height_1720)(vel_1720)
	return (model_1612, model_1665, model_1667, model_1720)
def Gaussian(mean, fwhm, height):
	return lambda x: height * np.exp(-4. * np.log(2) * (x - mean)**2 / fwhm**2)
def SkewnessTest(sampler_chain, num_Gauss, test_comp, test_limit):
	'''
	Tests the skewness of the inner 80% of the last 50 iterations of the given spectrum. 

	Parameters:
	sampler_chain - Markov chains from MCMC()
	num_Gauss - number of Gaussian components in the model
	test_comp - Model component to test: 0 for velocity, 1 for fwhm
	test_limit - Threshold used to determine whether or not the test is passed: 2

	Returns 'Pass' or 'Fail'
	'''
	Skewness_test = 'Pass'
	for component in range(num_Gauss):
		chain = sorted((sampler_chain[:,-50:-1,component * 6 + test_comp]).flatten())
		# find inner 80% of values
		chain_80 = chain[int(len(chain) / 10):int(9 * len(chain) / 10)]
		skewness = scipy.stats.skew(chain_80)
		if abs(skewness) > test_limit:
			Skewness_test = 'Fail'
	return Skewness_test
def ConvergenceTest(sampler_chain, num_Gauss, test_comp, test_limit):
	'''
	Tests the range of the inner 50% of the last 50 iterations of the given spectrum.

	Parameters:
	sampler_chain - Markov chains from MCMC()
	num_Gauss - number of Gaussian components in the model
	test_comp - Model component to test: 0 for velocity, 1 for fwhm
	test_limit - Threshold used to determine whether or not the test is passed: 2.5 km/sec (based on 
			extensive testing of synthetic spectra with velocity resolution ~0.3 km/sec and fwhm ~ 1-4 km/sec)

	Returns 'Pass' or 'Fail'
	'''
	Convergence_test = 'Pass'

	for component in range(num_Gauss):
		inner_50_range = corner.quantile((sampler_chain[:,-50:-1,component * 6 + test_comp]).flatten(), [0.75]) - corner.quantile((sampler_chain[:,-50:-1,component * 6 + test_comp]).flatten(), [0.25])
		if abs(inner_50_range) > test_limit:
			Convergence_test = 'Fail'
	return Convergence_test
# Run
def Main(source_name, vel_axes, spectra, rms, expected_min_fwhm, save_as_name):
	'''
	Performs Bayesian Gaussian Decomposition on velocity spectra of the 2 Pi 3/2 J = 3/2 ground state 
	transitions of OH.

	Parameters:
	source_name - unique identifier for sightline, used in plots, dictionaries etc.
	vel_axes - list of velocity axes: [vel_axis_1612, vel_axis_1665, vel_axis_1667, vel_axis_1720]
	spectra - list of spectra (brightness temperature or tau): [spectrum_1612, spectrum_1665, spectrum_1667, 
			spectrum_1720]
	rms - list of estimates of rms error in spectra: [rms_1612, rms_1665, rms_1667, rms_1720]. Used by AGD
	expected_min_fwhm - estimate of the minimum full width at half-maximum of features expected in the data 
			in km/sec. Used when categorising features as isolated or blended.
	save_as_name - unique identifier for sightline, used when saving plots, reporting results etc. (i.e. can 
		be longer and uglier than source_name)

	Returns parameters of Gaussian component(s): [vel_1, fwhm_1, height_1612_1, height_1665_1, height_1667_1, 
			height_1720_1, ..., _N] for N components
	'''
	plot_num = 0
	dBIC_threshold = -10 # based on R. E. Kass and A. E. Raftery. Bayes factors. Journal of the American Statistical Association 90, 773 (1995)
	number_of_lines = len(vel_axes)
	if number_of_lines == 3 or number_of_lines == 4:
		print '\n\n' + str(number_of_lines) + ' lines entered for source ' + save_as_name
	else:
		print 'Error loading data'

	#############
	# Load Data #
	#############

	data = {'source_name': source_name, 'vel_axis': {'1612': [], '1665': [], '1667': [], '1720': []}, 'spectrum': {'1612': [], '1665': [], '1667': [], '1720': []}, 'rms': {'1612': [], '1665': [], '1667': [], '1720': []}}

	[spectrum_1612, spectrum_1665, spectrum_1667, spectrum_1720] = spectra

	spectrum_1612[np.isnan(spectrum_1612)] = 0
	spectrum_1665[np.isnan(spectrum_1665)] = 0
	spectrum_1667[np.isnan(spectrum_1667)] = 0
	spectrum_1720[np.isnan(spectrum_1720)] = 0

	spectrum_1612[np.isinf(spectrum_1612)] = 0
	spectrum_1665[np.isinf(spectrum_1665)] = 0
	spectrum_1667[np.isinf(spectrum_1667)] = 0
	spectrum_1720[np.isinf(spectrum_1720)] = 0

	data['vel_axis']['1612']	= vel_axes[0]
	data['spectrum']['1612']	= spectrum_1612
	data['rms']['1612']			= rms[0]

	data['vel_axis']['1665']	= vel_axes[1]
	data['spectrum']['1665']	= spectrum_1665
	data['rms']['1665']			= rms[1]

	data['vel_axis']['1667']	= vel_axes[2]
	data['spectrum']['1667']	= spectrum_1667
	data['rms']['1667']			= rms[2]

	data['vel_axis']['1720']	= vel_axes[3]
	data['spectrum']['1720']	= spectrum_1720
	data['rms']['1720']			= rms[3]

	dv = abs(((data['vel_axis']['1665'][1] - data['vel_axis']['1665'][0]) + (data['vel_axis']['1667'][1] - data['vel_axis']['1667'][0])) / 2)

	######################
	# Identify Gaussians #
	######################

	grouped_velocities = IdentifyGaussians(data, expected_min_fwhm, dv)

	plt.figure()
	plt.plot(data['vel_axis']['1612'], data['spectrum']['1612'], color = 'blue', label = '1612 MHz')
	plt.plot(data['vel_axis']['1665'], data['spectrum']['1665'], color = 'green', label = '1665 MHz')
	plt.plot(data['vel_axis']['1667'], data['spectrum']['1667'], color = 'red', label = '1667 MHz')
	plt.plot(data['vel_axis']['1720'], data['spectrum']['1720'], color = 'cyan', label = '1720 MHz')
	for velocity_group in grouped_velocities:
		for velocity in velocity_group:
				plt.vlines(velocity, -0.5, 0.5, color = 'pink')
	plt.savefig('Models/Initial_guesses/' + str(save_as_name) + '.pdf')
	plt.close()

	#################
	# Fit Gaussians #
	#################

	null_BIC = BICnull(data)

	final_velocities = []
	
	for velocity_group in grouped_velocities:
		blended_model_parameters = []
		median_parameters = []
		dBIC_list = []
		new_velocity = []

		###### Re-order grouped velocities for further analysis ######
		if len(velocity_group) > 1: 
			for velocity in velocity_group:
				BIC_model, median_parameters, Convergence_test, Skewness_test = MCMC(data, [velocity], null_BIC,expected_min_fwhm, plot_num, save_as_name)
				plot_num += 1
				if BIC_model - null_BIC < dBIC_threshold:
					dBIC_list.append(BIC_model - null_BIC)
					new_velocity.append(velocity)
			sorted_velocities = [x for _,x in sorted(zip(dBIC_list, new_velocity))]
		else: # i.e. it's an isolated feature. This step is skipped and it's moved on
			sorted_velocities = velocity_group
		
		###### Test grouped velocities ######
		prev_BIC = null_BIC
		dBIC_test = -100
		Convergence_test = 'Pass'
		Skewness_test = 'Pass'
		accepted_velocities = []
		
		if len(sorted_velocities) > 1:
			for velocity in sorted_velocities:
				BIC_model, median_parameters, Convergence_test, Skewness_test = MCMC(data, np.append(accepted_velocities, velocity), prev_BIC, expected_min_fwhm, plot_num, save_as_name)
				plot_num += 1
				dBIC_test = BIC_model - prev_BIC

				######### v Significance test v #########

				sig_test = 'Pass'
				for component in range(len(median_parameters) / 6):
					if abs(median_parameters[2]) < data['rms']['1612'] and abs(median_parameters[component * 6 + 3]) < data['rms']['1665'] and abs(median_parameters[component * 6 + 4]) < data['rms']['1667'] and abs(median_parameters[component * 6 + 5]) < data['rms']['1720']:
						sig_test = 'Fail'

				######### ^ Significance test ^ #########

				if dBIC_test < dBIC_threshold and Convergence_test == 'Pass' and Skewness_test == 'Pass' and sig_test == 'Pass':
					accepted_velocities = np.append(accepted_velocities, velocity)
					prev_BIC = BIC_model

			if len(accepted_velocities) > 0: # length should be 6 if there's one component
				final_velocities = np.append(final_velocities, accepted_velocities)

		elif len(sorted_velocities) == 1:
			BIC_model, median_parameters, Convergence_test, Skewness_test = MCMC(data, sorted_velocities, null_BIC, expected_min_fwhm, plot_num, save_as_name)
			plot_num += 1
			dBIC_test = BIC_model - prev_BIC

			######### v Significance test v #########

			sig_test = 'Pass'
			if abs(median_parameters[2]) < data['rms']['1612'] and abs(median_parameters[3]) < data['rms']['1665'] and abs(median_parameters[4]) < data['rms']['1667'] and abs(median_parameters[5]) < data['rms']['1720']:
				sig_test = 'Fail'

			######### ^ Significance test ^ #########

			if dBIC_test < dBIC_threshold and Convergence_test == 'Pass' and Skewness_test == 'Pass' and sig_test == 'Pass':
				final_velocities = np.append(final_velocities, sorted_velocities)


	#### Final fit ####
	if len(final_velocities) > 0:
		plot_num = 'Final'
		BIC_model, final_parameters, Convergence_test, Skewness_test = MCMC(data, final_velocities, null_BIC, expected_min_fwhm, plot_num, save_as_name)

		return final_parameters
	else:
		return [0]
# Report Results
def ResultsReport(final_parameters, save_as_name):
	'''
	Generates a nice report of results
	'''
	if len(final_parameters) < 6:
		print '\n\No features identified for ' + save_as_name
	else:
		print '\n\Results for ' + save_as_name
		print '\tNumber of features identified: ' + str(len(final_parameters) / 6)
		print '\tFeature Parameters:'
		for feature in range(len(final_parameters)/6):
			print '\t\tfeature number ' + str(feature + 1) + ':'
			[vel, fwhm, height_1612, height_1665, height_1667, height_1720] = final_parameters[feature * 6:feature * 6 + 6]
			print '\t\t\tcentroid velocity = ' + str(vel) + ' km/sec'
			print '\t\t\tfull width at half-maximum = ' + str(fwhm) + ' km/sec'
			print '\t\t\t1612 MHz height = ' + str(height_1612)
			print '\t\t\t1665 MHz height = ' + str(height_1665)
			print '\t\t\t1667 MHz height = ' + str(height_1667)
			print '\t\t\t1720 MHz height = ' + str(height_1720)
